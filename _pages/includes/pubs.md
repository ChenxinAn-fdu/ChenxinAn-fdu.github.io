# Selected Publications
[POLARIS: A Post-Training Recipe for Scaling Reinforcement Learning on Advanced Reasoning Models](https://www.notion.so/POLARIS-A-POst-training-recipe-for-scaling-reinforcement-Learning-on-Advanced-ReasonIng-modelS-1dfa954ff7c38094923ec7772bf447a1) 

**Chenxin An**, Zhihui Xie, Xiaonan Li, Lei Li, Jun Zhang, Shansan Gong, Ming Zhong, Jingjing Xu, Xipeng Qiu, Mingxuan Wang, Lingpeng Kong

[Polaris ![](https://img.shields.io/github/stars/ChenxinAn-fdu/POLARIS?style=social)](https://github.com/ChenxinAn-fdu/POLARIS) \| SOTA RL training recipe for advanced reasoning models.


---

[L-Eval: Instituting Standardized Evaluation for Long Context Language Models](https://arxiv.org/pdf/2307.11088.pdf) (ACL 2024)

**Chenxin An**, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, Xipeng Qiu *((<span style="color:red;">OutstandingÂ paper</span>, ACL 2024))*

[L-Eval ![](https://img.shields.io/github/stars/OpenLMLab/LEval?style=social)](https://github.com/OpenLMLab/LEval) \| A comprehensive evaluation suite for long-context language models with 20 sub-tasks and optimized evaluation metric.

---


[Training-Free Long-Context Scaling of Large Language Models](https://arxiv.org/pdf/2402.17463) (ICML 2024)

**Chenxin An**, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, Lingpeng Kong

[ChunkLlama ![](https://img.shields.io/github/stars/HKUNLP/ChunkLlama?style=social)](https://github.com/HKUNLP/ChunkLlama) \| A training-free method to extend Llama 2 70B to 100k context length (x48 times). 

---

[Why Does the Effective Context Length of LLMs Fall Short?](https://arxiv.org/abs/2410.18745) (ICLR 2025)

**Chenxin An**, Jun Zhang, Ming Zhong, Lei Li, Shansan Gong, Yao Luo, Jingjing Xu, Lingpeng Kong

[STRING ![](https://img.shields.io/github/stars/HKUNLP/STRING?style=social)](https://github.com/HKUNLP/STRING) \| Interesting findings about the effective length growth of LLMs and new positional encodings STRING.

---

[CoNT: Contrastive Neural Text Generation](https://arxiv.org/abs/2205.14690) (NeurIPS 2022 Spotlight)

**Chenxin An**, Jiangtao Feng, Kai Lv, Lingpeng Kong, Xipeng Qiu, Xuanjing Huang

[CoNT ![](https://img.shields.io/github/stars/Shark-NLP/CoNT?style=social)](https://github.com/Shark-NLP/CoNT) \|A contrastive learning training method for improving autoregressive text generation.


https://github.com/Shark-NLP/CoNT

---

[Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective](https://arxiv.org/pdf/2310.11451.pdf) (ICLR 2024)

Ming Zhong, **Chenxin An**, Weizhu Chen, Jiawei Han, Pengcheng He

---

[Scaling laws of rope-based extrapolation](https://arxiv.org/pdf/2310.05209.pdf) (ICLR 2024)

Xiaoran Liu, Hang Yan, Shuo Zhang, **Chenxin An**, Xipeng Qiu, Dahua Lin

---

[Temporal Reasoning Transfer from Text to Video](https://arxiv.org/pdf/2410.06166.pdf) (ICLR 2025)

Lei Li, Yuanxin Liu, Linli Yao, Peiyuan Zhang, **Chenxin An**, Lean Wang, Xu Sun, Lingpeng Kong, Qi Liu

---

[Scaling Diffusion Language Models via Adaptation from Autoregressive Models](https://arxiv.org/pdf/2410.17891.pdf) (ICLR 2025)

Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, **Chenxin An**, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, Lingpeng Kong

---






