#  Publications (2023 - )

[Why Does the Effective Context Length of LLMs Fall Short?](https://arxiv.org/abs/2410.18745) 

**Chenxin An**, Jun Zhang, Ming Zhong, Lei Li, Shansan Gong, Yao Luo, Jingjing Xu, Lingpeng Kong

[STRING ![](https://img.shields.io/github/stars/HKUNLP/STRING?style=social)](https://github.com/HKUNLP/STRING) \| Interesting findings about the effective length of LLMs and new positional encoding methods for LLMs at inference stage.

---

[L-Eval: Instituting Standardized Evaluation for Long Context Language Models](https://arxiv.org/pdf/2307.11088.pdf) (ACL 2024)

**Chenxin An**, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, Xipeng Qiu *(Outstanding paper, ACL2024)*

[L-Eval ![](https://img.shields.io/github/stars/OpenLMLab/LEval?style=social)](https://github.com/OpenLMLab/LEval) \| A comprehensive evaluate suite for long-context language models with 20 sub-tasks and optimized evaluation metric.

---
[Training-Free Long-Context Scaling of Large Language Models](https://arxiv.org/pdf/2402.17463) (ICML 2024)

**Chenxin An**, Fei Huang, Jun Zhang, **Shansan Gong**, Xipeng Qiu, Chang Zhou, Lingpeng Kong

[ChunkLlama ![](https://img.shields.io/github/stars/HKUNLP/ChunkLlama?style=social)](https://github.com/HKUNLP/ChunkLlama) \| A training-free method to extend Llama 2 70B to 100k context length (x48 times). 

---

[Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective](https://arxiv.org/pdf/2310.11451.pdf) (ICLR 2024)

Ming Zhong, **Chenxin An**, Weizhu Chen, Jiawei Han, Pengcheng He

---

[Scaling laws of rope-based extrapolation](https://arxiv.org/pdf/2310.05209.pdf) (ICLR 2024)

Xiaoran Liu, Hang Yan, Shuo Zhang, **Chenxin An**, Xipeng Qiu, Dahua Lin

---





